from requests_html import HTMLSession
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse

session = HTMLSession()

def scrape_links_in_website(url):
    try:
        response = session.get(url)
        response.raise_for_status()  # Will raise HTTPError for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')

        links = set()
        for tag in soup.find_all('a', href=True):
            link = urljoin(url, tag['href'])
            if urlparse(link).netloc == urlparse(url).netloc:
                links.add(link)
        return links
    except Exception as e:
        print(f"Failed to scrape links from {url}: {e}")
        return set()

def scrape_email_from_website(url):
    visited = set()
    emails = set()
    pages_to_visit = scrape_links_in_website(url)
    visited.add(url)

    while pages_to_visit:
        page_url = pages_to_visit.pop()
        if page_url in visited:
            continue
        visited.add(page_url)

        try:
            response = session.get(page_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
            found_emails = set(re.findall(email_pattern, soup.get_text()))
            emails.update(found_emails)
            print(f"Scraped {len(found_emails)} emails from {page_url}")
        except Exception as e:
            print(f"Failed to scrape emails from {page_url}: {e}")
            continue

    return list(emails)

url = 'https://www.thekooples.com/'
result = scrape_email_from_website(url)
print(result)
