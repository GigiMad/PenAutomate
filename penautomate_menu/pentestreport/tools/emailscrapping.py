from requests_html import HTMLSession
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import json
import os

# Setup paths for JSON files
json_input_path = os.path.join("penautomate_menu", "pentestreport", "pentestdata.json")
json_output_path = os.path.join("penautomate_menu", "pentestreport", "pentestreport.json")

# Functions to load and save data
def load_existing_data(filepath):
    try:
        with open(filepath, 'r') as file:
            return json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_data(filepath, data):
    with open(filepath, 'w') as file:
        json.dump(data, file, indent=4)

# Load the domain from the JSON file
data = load_existing_data(json_input_path)
domain = data.get("Company Domain", "")
url = f'https://www.{domain}'

session = HTMLSession()

def scrape_links_in_website(url):
    try:
        response = session.get(url)
        response.raise_for_status()  # Will raise HTTPError for bad responses
        soup = BeautifulSoup(response.content, 'html.parser')

        links = set()
        for tag in soup.find_all('a', href=True):
            link = urljoin(url, tag['href'])
            if urlparse(link).netloc == urlparse(url).netloc:
                links.add(link)
        return links
    except Exception as e:
        print(f"Failed to scrape links from {url}: {e}")
        return set()

def scrape_email_from_website(url):
    visited = set()
    emails = set()
    pages_to_visit = scrape_links_in_website(url)
    visited.add(url)

    while pages_to_visit:
        page_url = pages_to_visit.pop()
        if page_url in visited:
            continue
        visited.add(page_url)

        try:
            response = session.get(page_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')
            email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
            found_emails = set(re.findall(email_pattern, soup.get_text()))
            emails.update(found_emails)
            print(f"Scraped {len(found_emails)} emails from {page_url}")
        except Exception as e:
            print(f"Failed to scrape emails from {page_url}: {e}")
            continue

    return list(emails)

# Perform email scraping
emails_found = scrape_email_from_website(url)

# Load existing results and merge new data
results = load_existing_data(json_output_path)
results["email found"] = emails_found

# Save the updated results back to JSON
save_data(json_output_path, results)
