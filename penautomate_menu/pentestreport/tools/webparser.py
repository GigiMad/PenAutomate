import requests
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import json
import os

def load_existing_data(filepath):
    try:
        with open(filepath, 'r') as file:
            return json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_data(filepath, data):
    with open(filepath, 'w') as file:
        json.dump(data, file, indent=4)

def discover_urls(url):
    discovered_urls = []
    response = requests.get(url)
    if response.status_code == 200:
        soup = BeautifulSoup(response.text, "html.parser")
        for anchor_tag in soup.find_all("a"):
            href = anchor_tag.get("href")
            if href:
                absolute_url = urljoin(url, href)
                discovered_urls.append(absolute_url)
    return discovered_urls

def scan_url(url):
    vulnerabilities = {}
    if is_sql_injection_vulnerable(url):
        vulnerabilities["SQL injection vulnerability"] = "Injecting SQL code into input fields"
    if is_xss_vulnerable(url):
        vulnerabilities["Cross-site scripting (XSS) vulnerability"] = "Injecting malicious scripts into input fields"
    if has_insecure_configuration(url):
        vulnerabilities["Insecure server configuration"] = "Exploiting insecure communication protocols"
    return vulnerabilities

def is_sql_injection_vulnerable(url):
    payload = "' OR '1'='1"
    response = requests.get(url + "?id=" + payload)
    return "error" in response.text or "warning" in response.text

def is_xss_vulnerable(url):
    payload = "<script>alert('XSS')</script>"
    response = requests.get(url + "?input=" + payload)
    return payload in response.text

def has_insecure_configuration(url):
    return not url.startswith("https")

def scan_website(url):
    results = {}
    discovered_urls = discover_urls(url)
    for page_url in discovered_urls:
        vulnerabilities = scan_url(page_url)
        if vulnerabilities:
            results[page_url] = vulnerabilities
    return results

def main():
    json_input_path = os.path.join("penautomate_menu", "pentestreport", "pentestdata.json")
    json_output_path = os.path.join("penautomate_menu", "pentestreport", "pentestreport.json")
    data = load_existing_data(json_input_path)
    ips = [data.get(f"IP{i}") for i in range(1, 4) if data.get(f"IP{i}")]
    all_results = {}
    for ip in ips:
        url = f"http://{ip}"
        print(f"Scanning {url}")
        results = scan_website(url)
        all_results[ip] = results

    existing_results = load_existing_data(json_output_path)
    if "webparser results" not in existing_results:
        existing_results["webparser results"] = {}
    existing_results["webparser results"].update(all_results)
    save_data(json_output_path, existing_results)

if __name__ == "__main__":
    main()