import requests
from bs4 import BeautifulSoup
import json
import os

# Setup paths for JSON files
json_input_path = os.path.join("penautomate_menu", "pentestreport", "results", "pentestdata.json")
json_output_path = os.path.join("penautomate_menu", "pentestreport", "results", "pentestreport.json")

# Functions to load and save data
def load_existing_data(filepath):
    try:
        with open(filepath, 'r') as file:
            return json.load(file)
    except (FileNotFoundError, json.JSONDecodeError):
        return {}

def save_data(filepath, data):
    with open(filepath, 'w') as file:
        json.dump(data, file, indent=4)

# Load the domain from the JSON file
data = load_existing_data(json_input_path)
domain = data.get("Company Domain", "")
url = f'https://www.{domain}'


def get_social_media_links(url):
    try:
        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'html.parser')
        links = []
        # Définir les domaines de réseaux sociaux populaires
        social_media_domains = [
            'facebook.com', 'twitter.com', 'linkedin.com',
            'instagram.com', 'youtube.com', 'pinterest.com',
            'snapchat.com', 'tiktok.com', 'reddit.com', 'tumblr.com'
        ]
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and any(social in href for social in social_media_domains):
                links.append(href)
        return links
    except requests.RequestException as err:
        return str(err)

# Perform email scraping
sm_fnoud = get_social_media_links(url)

# Load existing results and merge new data
results = load_existing_data(json_output_path)
results["Social Media Links"] = sm_fnoud

# Save the updated results back to JSON
save_data(json_output_path, results)

social_media_links = get_social_media_links(url)
print(social_media_links)